1.1 What is Cloud?
    - Have large amount of data
    - Not easy to move data around
    - So bring Computer Cycles closer to data

    - Today's cloud computing is very close to Data Processing

3.1 Map Reduce:
    - Map split the task into Maps
    - Reduces based on the key.
    - Arranges similar key to same reduce task.
    - Eg: Word Count: Hello world hi world
    - Map Task: 4 words with 1 count to each word
    - Word: Hello world hi world
            1     1     1  1   

    - Now Hello & hi goes to Reducer 1;
    - world and world goes to Reduces 2;

    - Reduce task is called once for each key
    - So number of keys = number of reduce tasks

    Eg: Count of URL access frequency.
    How much percent a URL is being visited from total URLs
    Map 1           : Process all URLs and put a count(1) next to it; <URL, 1>
    Multiple Red 1  : With URL as key, count total number of time a URL is visited
                      Each URL could be given to a reducer. <URL, count>

    Now we have to find the UNIQUE URLs
    Map 2           : Process <URL, URL_count> and produce <1, <URL, URL_Count>
    Single Red 1    : Take all the results from map and sum it to find total uniq URLs
    Result          : <URL, URL_count / total Uniq URL count>

3.3. Programming Map Reduce:
    For a user,
    - Write Map function; 
    - Write Reduce function
    - Submit Job and wait for result

    Internally:
    1. How to Parallelize the Map tasks
        - Easy. All Map tasks are independent of each other

    2. Transfer results from Map to Reduce
        - All output records with same Key are given to same reduce
        - Use paritioning function

    3. Parallelize Reduce
        - Easy. Each reduce tasks is independent of each other

    4. Implement storage for Map input and Map Output (Reduce Input) and Reduce Output
        - GFS, HDFS etc
        - Map Input: Got from Distributed File System
        - Map Output: Goes to LOCAL DISK
        - Red Input: Read from Local Disk
        - Red Output: Goes to DFS

        - Using the local file system makes things FASTER.
        - It avoid high overhead of the underlying DFS.

    - REDUCE TASKS can't start BEFORE MAP tasks are complete. True most of the time

    YARN:
        - Glboal Resource Manager:
            Scheduling
        - Per server Node Manager
            Each server runs a node manager
            Responsible for all Server specific management
        - Per application Job Manager
            Negotiate Containers with Resource Manager
            Communicates with Node manager to find if any of them have died. If so reschedule them

3.4 Fault Tolerance
    Speculative Execution:
        The slowest task slows all other processes just delaying the job completion
        So keep track of each tasks
        Replicate the one that is running the slowest in some other problem
        Once one of the replicated job is complete, the task gets mared as complete.
        And the slower replica gets killed.

    Locality
        - A rack can have multiple machines
        - A scheduler determines where the task should be run
            - In a machine that contains a replica
            - OR on the same rack so that it is faster
            - OR Anywhere

Week 2:
1 Gossip Protocol:
    1.1 Multicast Problem
        Requirements for multicast problem:
        - Fault tolerant and reliable
            - Nodes could crash,
            - Packets could get lost
        - Should be scalable

        If you want to send to 'n' nodes, if we do in a linearly, then complexity is O(n)
        So have a tree based solution.

        But if a middle node fails, then all its child won't get the information.
        - So we should constantly be repairing the tree.
            - Use ACKs and NACKs (Negative Acks)
        
        SRM:
            - Uses NAKs
            - But amount of ACKs and NAKs could implode (very large number). 
            - So add random delays and
            - exponential backoffs when multiple NACKs should be sent to avoid NAK storms.

        RMTP:
            - Uses ACKs
            - But send a digest that contains all the ACKs
            - ACKs are sent only to designated receivers.
                - This is done to avoid ACK storms. 
            - Only the designated receivers receiver the ACK and they forward the message
              down the tree.

        - Even with delays and exponential backoff, large number of ACKs and NAKs would get sent
        - So it is not scalable. As nodes increase even ACKs and NAKs increase.
        Still we don't have a good solution in the above approaches.

    1.2 Gossip Protocol - EPIDEMIC BASED APPROACH
        - A node selects a random set of nodes (b) and multicasts data to them
        - Sends data periodically every few seconds to random (b) nodes
        - UDP transmission is used.
        - Gossip either a subset of messages or recently received ones or high priority ones

        - PUSH vs PULL
            Pull:
            - Ask if any new broadcasts are there with another node
            - Much faster in the second half.
            - So better to be used in later rounds

        - A nice way of multicast implementation
        - Fault tolerant

    1.3 Gossip Analysis:
        - If N/2 nodes in a subnet, then the rounter could have N/2 gossip messages to be sent.
        - Fix: Higher probability to stay within a subnet.

    2.1 Group Membership List
        - A list of processes is maintained.
        - Each process has a list of members that are active (non faulty)
        - This LIST should be upto date as Processes JOIN, LEAVE, on FAILURE
        - If a process fails, we should detect it and notify everyone else.
        
        - Should have two components
            1. Failure Detection
            2. Dissemination

        FOCUS ON ALMOST COMPLETE LIST

        Failure Detection:
            - Require 100% COMPLETENESS
                - When a process fails that process is detected, eventually, by at least one other non-faulty process.
                - When we have a failure, we MUST detect it

        Slightly less 100% ACCURACY is acceptable
        But we should have 100% COMPLETENESSS
            - When a process is detected as having failed, that process has in fact failed.
            - In other words, it says that there are no mistaken failure detections or there are no false positives.
            - Because even if we kick out a NON FAULTY guy, it's okay. He will rejoin

    2.2 Heartbeating
        Centralized Heartbeat:
            - A central system to which all process sends heartbeats
            - Problem: The central system could get overloaded / fail

        Ring Heartbeating:
            - Send heart beat to both left and right neighbors
            - Both could fail

        All to All Heartbeating
            - To all other processes in the system
            - Equal load per number
            - Protocol is complete

            Problem:
            - What if one process receives messages very slowly
            - Then it could marked that everything has failed.
            - So very high false positive

    2.3 Gossip Style Membership
        Various factors to be considered
        1. False Positive Rate Pmistake
        2. Detection Time
        3. Bandwidth
        4. Tgossip time
        5. Tfail
        5. Tcleanup

    2.4
        Accuracy: Probability of a Mistake in Time t: PM(t)

        All to ALL Heatbeating:
            - Every T unit, send L = N/T heartbeats
            - Load = N heatbeat sent every T units

        Gossip Style:
            tg: the period at which gossips are sent out.
            Load on each member per gossip is Order N as every gossip message will have the entire
            list.
            A gossip takes LOG(N) rounds to propagate.
            - T = Log N * tg
            - L = N/tg = N*logN / T

    2.5 Another Probabilistic Approach
        - Instead of Heartbeats, use Pings.
        - Send indirect ping

        - Pick one process at Random and send a Ping
        - If ACK received then done.
        - Else Send an indirect ping and wait for an indirect ACK
            - Indirect ping is sent bo 'k' other processes
            - Even if One indirect ACK is received then done
        - If no indirect ACK then remove the process

        Hearbeating vs Swim:

    2.6 Dissemination and Suspension   
        - Once failure is detected that info should be sent to everyone else

        - Piggy back on failure detector messages
            - Useful when SWIM style is used.

        - Suspension:
            - Insted of directly moving to failed state, move to Suspect state
            - While in suspect state if there is a message from Pj then move back to active state

            - Problem:
                Piggyback between suspected and active state.
                - So have Incarnation numbers.
                - Only Pj can Increment the Incarnation number
                - If someone is suspecting Pj, then Pj will increment its Incarnation number and
                  start sending it out along with ALIVE.

Week 3:
P2P Systems:
    - Unstructured P2P systems.
        - Napster, Gnutella

    - Structured P2P systems
        - Chord, Pastry

    NAPSTER:
        - Server stores where the files are located

    Gnutella
        - Five types of message:
            Query, Queryhit, Push, Ping, and Pong

        - No server. All Peers
        - TTL-Restricted

        - Uses Descriptor ID and previous history of messages that are forwarded to avoid
          re-forwarding the same message.

        QUERY HIT:
        - Every PEER keeps tracks of recent query messages and which Peer it got the query from
          This is used for REVERSING the query search i.e. for Query Hit

        TTL:
        - Whenever a message is forwarded by a peer to its neighboring peers,
          TTL is decremented by one.
        - When the TTL hits zero, you do not forward the message anymore.
        - The TTL is set to be a finite number, so that essentially a message doesn't keep circulating around the overlay graph forever.

        Transfer Files:
        - HTTP is used.
        - GET message is used to transfer the files
        - If behind FIREWALL HTTP gets blocked.
          So same concept of Push and Reverse Path Lookup is used.

        PROBLEMS:
        - Ping pong constituted 50% of traffic
          - Even though you recieve multiple pongs, you consolidate and send pongs to your neighbors

    FastTrack:
        - Just like Gnutella but few peers are Supernodes
        - So query just by searching the Supernode instead of the entire network

    BitTorrent:
        - Choking: To make sure too much of upload doesn't happen

    Chord:
        - Distributed Hash table (DHT)
        Performance Concerns:
            - Load Balancing
                - Virtual Nodes (Virtual Ring) assumption to ensure load balancing
            - Fault Tolerance

        - Uses CH (Consistent Hashing) on peer's address
            - Get the hash value and truncate to 'm' bits

    Pastry:
        - Each nodes knows few successors and few predecessors
        -  This is sort of like hypercube routing.
        - So routing is based on prefix matching and turns out to be O(log(N)) because a hypercube with N points is O(log(N)) in diameter.
        - In addition Pastry, unlike Chord, also pays attention to the underlying network.
        - This makes the neighbor edges to be short in the underlying neighboring topology as well. 

        - Select the neighbor with shortest-round-trip time
        - So early hops are much shorter and later hops are much longer

    Kelips:
        - Gives Constant Lookup Cost
        - We don't use a VIRTUAL RING instead use an AFFINITY GROUP

        - Unlike in Chord and Pastry, the files do not get stored at the nodes in the affinity groups.
        - Instead, they get stored at whichever peer uploaded them just like Napster or Gnutella.
        - Instead, what gets stored in an affinity group is meta-information, so you decouple the file replication location from the file querying.

        - You are replicating the info of a file SO WELL that at any points from any of your
          neighbors you can easily get to the file

    Pastry vs Chord vs Kelips
        - So Kelips uses slightly more memory than Chord or Pastry and slightly more background bandwidth, to keep neighbors a fresh, but it has a much shorter lookup cost, which is O(1).
        So while Chord or Pastry have O(log(N)) for all for both memory and lookup, Kelips has O(square root(N)) for memory and O(1) for lookup cost.

Week 4:
    4.1.1 Why Key/Value Store:
    - Data is Large and Unstructured
    - Don't need Joins.
    - Foreign key is rarely used.
    - Write HEAVY
    - RDBMS is useful for READ HEAVY

    NoSQL: Not Only SQL
        - Colum

    4.1.2 Cassandra
        - Developed by FB
        COORDINATOR:
            - The client sends its queries to one of the servers in the system.
            - It need not be one of the replicas. This server is called the coordinator.
            - The coordinator need not be on a per data center basis, it could be on a per client basis or on a per query basis, it doesn't really matter.
            - When Client sends a query, coordinator just forwards it to all or some of the replicas.
            - IMP:  The coordinator again may be per-key, per-client, or per-query.

        PARTITIONER:
            - Mapping from KEY to a SERVER
            - It is used by the Coordinator to find the REPLICA servers to which a query should be forwarded.
            - Different Types:
              Simple Strategy and Network Topology Strategy

        SNITCHES:
            - To map IP addresses to RACKS and Data Centers

        Writes:
            - Hinted Handoff Mechanism
              - Happens when Client sends a write to a coordinator and the replicas are down
            - The coordinator receives his write, uses the PARTITIONER to find out which of the replicas that map to that particular key
            - Coordinator keeps a copy of the WRITE till the replica that is down comes up
            - This is done for a small duration.

            - Replica logs the write to a diskfile to protect from failures

        MEMTABLE:
            - It is an IN-MEMORY REPRESENTATION of multiple key-value pairs
            - It is the one that holds all key-value pairs
            - it is maintained as WRITE BACK CACHE instead of WRITE THROUGH CACHE

            WriteBack Cache - Storing temporarily in memory
            WriteThrough Cache - Writing directly to disk. Slower

        Access Table OR SSTable:
            - a ROW could be split across multiple access tables.
            - Once Memtable becomes full or reached a timeout, you flush its contents to disk
            - CREATE an ASSIST table or a SORTED String Table. This is stored on the disk.
            - Searching for a KEY in this data file could take long as it has KEY and VALUES

            - So an INDEX file is mainted to Store Key and position of data 
            - Use BLOOM FILTER to store efficiently
            - This helps in telling if the key is present or not

        Membership:
            - Every server maintains a LIST OF ALL OTHER SERVERS in the cluster
            - Uses Gossip style memebership
            - Suspicion mechanism is used to ensure ACCURACY

    4.1.3 CAP Theorem
        Cassandra favors Availability than Consistency (can tolerate a hit)
        
        Quorum:
            - More than 50% of replica should response
            - ANY
            - ALL
            - One : At least one replica. Can't be the coordinator
            - Quorum
            - Local_Quorum
            - Each_Quorum

    4.1.4 Consistency Spectrum
        - Per Key Sequential

        - CRDT - Commutative Replicated Data Type
            - Writes need not be ordered.
            - Writes in any order will give the same result

        - Red-Blue Consistency
            - Blue ops are COMMUTATIVE
            - Red ops should be executed in the same order

        - Causal Consistency:

    4.1.5 HBase
        - API functions:
            - Get/Put
            - Scan (row range, filter)
            - MultiPut

        - Prefers Consistency over Availability

        - HBase Table
            - Split into REGIONS
              Regions are a collection of ROWS
            - You don't want to store the entire ROW. So you split it into REGIONS
            - Column Family:
                - Subset of Columns with similar query pattern

            - One STORE per combination of ColumnFamily + Region

    4.2 Time and Ordering
    4.2.1 Why Synchronization:
        - Violation of Correctness:
            - When you miss the bus
        - Violation of Fairness:
            - When you wait too long for the bus

        - Each system has its own Clock
        - It is easy to ORDER events within a PROCESS
        - But it is difficult to ORDER events ACROSS processes

        Clock Skew vs Clock Drift
        - Clock Skew
            - Difference in Values of two clock
        - Clock Drift:
            - Difference in speed

    4.2.2 Cristian's Algorithm
        - External Synchronization
        - By the time we get the response from External Server, time would have moved on

        IMP:
        - NEVER change the CLOCK to old value
        - Can change the Speed of clock (Both Increase and Decrease)
    
    4.2.3 NTP
        - One of the standard

    4.2.4 Lamport Timestamp
        - Alternative to Synchronizing clocks
        - Order the events

    4.2.5 Vector Clocks
