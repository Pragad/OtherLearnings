1.1 What is Cloud?
    - Have large amount of data
    - Not easy to move data around
    - So bring Computer Cycles closer to data

    - Today's cloud computing is very close to Data Processing

3.1 Map Reduce:
    - Map split the task into Maps
    - Reduces based on the key.
    - Arranges similar key to same reduce task.
    - Eg: Word Count: Hello world hi world
    - Map Task: 4 words with 1 count to each word
    - Word: Hello world hi world
            1     1     1  1   

    - Now Hello & hi goes to Reducer 1;
    - world and world goes to Reduces 2;

    - Reduce task is called once for each key
    - So number of keys = number of reduce tasks

    Eg: Count of URL access frequency.
    How much percent a URL is being visited from total URLs
    Map 1           : Process all URLs and put a count(1) next to it; <URL, 1>
    Multiple Red 1  : With URL as key, count total number of time a URL is visited
                      Each URL could be given to a reducer. <URL, count>

    Now we have to find the UNIQUE URLs
    Map 2           : Process <URL, URL_count> and produce <1, <URL, URL_Count>
    Single Red 1    : Take all the results from map and sum it to find total uniq URLs
    Result          : <URL, URL_count / total Uniq URL count>

3.3. Programming Map Reduce:
    For a user,
    - Write Map function; 
    - Write Reduce function
    - Submit Job and wait for result

    Internally:
    1. How to Parallelize the Map tasks
        - Easy. All Map tasks are independent of each other

    2. Transfer results from Map to Reduce
        - All output records with same Key are given to same reduce
        - Use paritioning function

    3. Parallelize Reduce
        - Easy. Each reduce tasks is independent of each other

    4. Implement storage for Map input and Map Output (Reduce Input) and Reduce Output
        - GFS, HDFS etc
        - Map Input: Got from Distributed File System
        - Map Output: Goes to LOCAL DISK
        - Red Input: Read from Local Disk
        - Red Output: Goes to DFS

        - Using the local file system makes things FASTER.
        - It avoid high overhead of the underlying DFS.

    - REDUCE TASKS can't start BEFORE MAP tasks are complete. True most of the time

    YARN:
        - Glboal Resource Manager:
            Scheduling
        - Per server Node Manager
            Each server runs a node manager
            Responsible for all Server specific management
        - Per application Job Manager
            Negotiate Containers with Resource Manager
            Communicates with Node manager to find if any of them have died. If so reschedule them

3.4 Fault Tolerance
    Speculative Execution:
        The slowest task slows all other processes just delaying the job completion
        So keep track of each tasks
        Replicate the one that is running the slowest in some other problem
        Once one of the replicated job is complete, the task gets mared as complete.
        And the slower replica gets killed.

    Locality
        - A rack can have multiple machines
        - A scheduler determines where the task should be run
            - In a machine that contains a replica
            - OR on the same rack so that it is faster
            - OR Anywhere

Week 2:
1 Gossip Protocol:
    1.1 Multicast Problem
        Requirements for multicast problem:
        - Fault tolerant and reliable
            - Nodes could crash,
            - Packets could get lost
        - Should be scalable

        If you want to sent to 'n' nodes, if we do in a linearly, then complexity is O(n)
        So have a tree based solution.

        But if a middle node fails, then all its child won't get the information.
        - So we should constantly be repairing the tree.
            - Use ACKs and NACKs (Negative Acks)
        
        SRM:
            - Uses NAKs
            - uses random delays and
            - exponential backoffs when multiple NACKs should be sent to avoid NAK storms.

        RMTP:
            - Uses ACKs
            - ACKs are sent only to designated receivers.

    1.2 Gossip Protocol
        - A node selects a random set of nodes (b) and multicasts data to them
        - UDP transmission is used.
        - Sends data periodically every few seconds to random (b) nodes

        - A nice way of multicast implementation
        - Fault tolerant

    2.1 Group Membership List
        - A list of processes is maintained.
        - If a process fails, we should detect it and notify everyone else.
        
        Failure Detection:
            - Require 100% Completeness
                - When a process fails that process is detected, eventually, by at least one other non-faulty process.
                - When we have a failure, we MUST detect it
            - Slightly less 100% Accuracy is acceptable
                - When a process is detected as having failed, that process has in fact failed.
                - In other words, it says that there are no mistaken failure detections or there are no false positives.
                - Because even if we kick out a NON FAULTY guy, it's okay. He will rejoin

    2.2 Heartbeating
        Centralized Heartbeat:
            - A central system to which all process sends heartbeats
            - Problem: The central system could get overloaded / fail

        Ring Heartbeating:
            - Send heart beat to both left and right neighbors
            - Both could fail

        All to All Heartbeating
            - To all other processes in the system
            - Equal load per number
            - Protocol is complete

            Problem:
            - What if one process receives messages very slowly
            - Then it could marked that everything has failed.
            - So very high false positive

    2.3 Gossip Style Membership
