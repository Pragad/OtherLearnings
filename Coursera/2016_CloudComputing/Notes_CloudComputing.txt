1.1 What is Cloud?
    - Have large amount of data
    - Not easy to move data around
    - So bring Computer Cycles closer to data

    - Today's cloud computing is very close to Data Processing

3.1 Map Reduce:
    - Map split the task into Maps
    - Reduces based on the key.
    - Arranges similar key to same reduce task.
    - Eg: Word Count: Hello world hi world
    - Map Task: 4 words with 1 count to each word
    - Word: Hello world hi world
            1     1     1  1   

    - Now Hello & hi goes to Reducer 1;
    - world and world goes to Reduces 2;

    - Reduce task is called once for each key
    - So number of keys = number of reduce tasks

    Eg: Count of URL access frequency.
    How much percent a URL is being visited from total URLs
    Map 1           : Process all URLs and put a count(1) next to it; <URL, 1>
    Multiple Red 1  : With URL as key, count total number of time a URL is visited
                      Each URL could be given to a reducer. <URL, count>

    Now we have to find the UNIQUE URLs
    Map 2           : Process <URL, URL_count> and produce <1, <URL, URL_Count>
    Single Red 1    : Take all the results from map and sum it to find total uniq URLs
    Result          : <URL, URL_count / total Uniq URL count>

3.3. Programming Map Reduce:
    For a user,
    - Write Map function; 
    - Write Reduce function
    - Submit Job and wait for result

    Internally:
    1. How to Parallelize the Map tasks
        - Easy. All Map tasks are independent of each other

    2. Transfer results from Map to Reduce
        - All output records with same Key are given to same reduce
        - Use paritioning function

    3. Parallelize Reduce
        - Easy. Each reduce tasks is independent of each other

    4. Implement storage for Map input and Map Output (Reduce Input) and Reduce Output
        - GFS, HDFS etc
        - Map Input: Got from Distributed File System
        - Map Output: Goes to LOCAL DISK
        - Red Input: Read from Local Disk
        - Red Output: Goes to DFS

        - Using the local file system makes things FASTER.
        - It avoid high overhead of the underlying DFS.

    - REDUCE TASKS can't start BEFORE MAP tasks are complete. True most of the time

    YARN:
        - Glboal Resource Manager:
            Scheduling
        - Per server Node Manager
            Each server runs a node manager
            Responsible for all Server specific management
        - Per application Job Manager
            Negotiate Containers with Resource Manager
            Communicates with Node manager to find if any of them have died. If so reschedule them

3.4 Fault Tolerance
    Speculative Execution:
        The slowest task slows all other processes just delaying the job completion
        So keep track of each tasks
        Replicate the one that is running the slowest in some other problem
        Once one of the replicated job is complete, the task gets mared as complete.
        And the slower replica gets killed.

    Locality
        - A rack can have multiple machines
        - A scheduler determines where the task should be run
            - In a machine that contains a replica
            - OR on the same rack so that it is faster
            - OR Anywhere

Week 2:
1 Gossip Protocol:
    Multicast:
        - Generally at Application layer
        - Most of the multicasts that are out there today are at Application Level
        - But there are few multicast implementations that are at IP level
            Eg: IPmulticast
    1.1 Multicast Problem
        Requirements for multicast problem:
        - Fault tolerant and reliable
            - Nodes could crash,
            - Packets could get lost
        - Should be scalable

        If you want to send to 'n' nodes, if we do in a linearly, then complexity is O(n)
        So have a tree based solution.

        But if a middle node fails, then all its child won't get the information.
        - So we should constantly be repairing the tree.
            - Use ACKs and NACKs (Negative Acks)
        
        SRM:
            - Uses NAKs
            - But amount of ACKs and NAKs could implode (very large number). 
            - There could be very large number of ACKs and NACKs
            - So the protocol adds random delays to send out NACKs and
            - exponential backoffs when multiple NACKs should be sent to avoid NAK storms.

        RMTP:
            - Uses ACKs
            - But send a digest that contains all the ACKs
            - Ack storms can be avoided by sending ACKs are sent only to designated receivers
                - This is done to avoid ACK storms. 
            - Only the designated receivers receiver the ACK and they forward the message
              down the tree.

        IMP:
        - Even with delays and exponential backoff, large number of ACKs and NAKs would get sent
        - So it is not scalable. As nodes increase even ACKs and NAKs increase.
        Still we don't have a good solution in the above approaches.

    1.2 Gossip Protocol - EPIDEMIC BASED APPROACH
        - A node selects a random set of nodes (b) and multicasts data to them
        - Sends data periodically every few seconds to random (b) nodes
        - UDP transmission is used.
        - Gossip either a subset of messages or recently received ones or high priority ones

        - PUSH vs PULL
            Pull:
            - Ask if any new broadcasts are there with another node
            - Much faster in the second half.
            - So better to be used in later rounds

        - A nice way of multicast implementation
        - Fault tolerant

    PROJ:
    1.3 Gossip Analysis:
        - If N/2 nodes in a subnet, then the rounter could have N/2 gossip messages to be sent.
        - Fix: Higher probability to stay within a subnet.
    
    1.4 Gossip Implementations:
        - Tree based multicast protocols are not as EFFICIENT as Gossip

    2.1 Group Membership List
        - We will consider CRASH-STOP / FAIL-STOP process failures
            i.e If a process fails, it doesn't recover from it.
        - Another type is CRASH-RECOVERY / FAIL-RECOVERY MODEL

        IMP:
        - We have a large group of Processes and each are running the same algorithm / Protocol
        - Each process / a group of process maintains a MEMBERSHIP LIST
        - MEMBERSHIP List: Contains list of NON-FAULTY processees.
        - A list of processes are maintained.
        - Each process has a list of members that are active (non faulty)

        - Membership List gets accessed by Applications like
            Gossip, Overlays etc

        - GOAL is to build Membership Protocol that keeps the Membership list UPTO DATE
            - as they JOIN
            - as they FAIL
            - as they LEAVE

        - If a process fails, we should detect it and notify everyone else.
        - Membership list can be a COMPLETE LIST:
        COMPLETE LIST:
            - Consistent at ALL TIMES across all the processes in the list
            Eg: Virtual Synchrony

        - Should have two components
            1. Failure Detection
            2. Dissemination
                - Can also be used to disseminate information about process joins and process leave

        We will FOCUS ON ALMOST COMPLETE LIST
            - Gossip-style, SWIM
            - Weakly Consistent

        OUR GOAL:
            Failure Detection:
            - Consider two processes Pi and Pj. When Process Pj crashes, we have to make sure
              at least one NON-FAULTY PROCESS detects that Pj has failed.
            Dissemination:
            - Once a non faulty process has detected that Pj has failed, the information 
              should be disseminated to Pi

        Failure Detection:
            - Require 100% COMPLETENESS
                - When a process fails that process is detected, eventually, by at least one other non-faulty process.
                - When we have a failure, we MUST detect it

        Slightly less 100% ACCURACY is acceptable
        But we should have 100% COMPLETENESSS
            - When a process is detected as having failed, that process has in fact failed.
            - In other words, it says that there are no mistaken failure detections or there are no false positives.
            - Because even if we kick out a NON FAULTY guy, it's okay. He will rejoin

    2.2 Failure Detectors: How to design a Failure Detection Model
        - Completeness
        - Accuracy
        - Speed
            - Time until some process detects the failure
        - Scale
            - Equal load on all memebers
            - Low overall network load
            - Avoid bottlenecks

        - CANNOT build a network that is 100% Completeness and 100% Accuracy

    Heartbeating:
        Centralized Heartbeat:
            - A central system to which all process sends heartbeats

            PROJ
            Heartbeat:
                - Just a sequence number
                - Every time a process Pi sends and heartbeat to Pj, it increments its local
                  sequence number
                - Heartbeats are sent every 't' seconds
                - Pj keeps track of when the last heartbeat was sent

            Problem:
                - The central system could get overloaded / fail

        Ring Heartbeating:
            - All processes are organized in a virtual ring
            - Send heart beat to both left and right neighbors

            Problem:
            - Both could fail
            - Repairing the RING

        All to All Heartbeating
            - To all other processes in the system
            - Equal load per number (Even though the load is high)
            - Protocol is complete

            Problem:
            - What if one process receives messages very slowly
            - Then it could mark(notify everyone else) that the process has failed.
            - So very high false positive

    2.3 Gossip Style Membership
        Various factors to be considered
        1. False Positive Rate Pmistake
        2. Detection Time
        3. Bandwidth
        4. Tgossip time
        5. Tfail
        5. Tcleanup

    2.4 What is the best failure detector
        Completeness: Should be Guarenteed always
        Accuracy: Probability of a Mistake in Time t: PM(t)

        All to ALL Heatbeating:
            - Every T unit, send L = N/T heartbeats
            - Load = N heatbeat sent every T units

        Gossip Style:
            tg: the period at which gossips are sent out.
            Load on each member per gossip is Order N as every gossip message will have the entire
            list.
            A gossip takes LOG(N) rounds to propagate.
            - T = Log N * tg
            - L = N/tg = N*logN / T

        All to All Hearbeating and Gossip-based are SUB-OPTIMAL
            - BECAUSE: they combine both Failure Detection and Dissemination component.
            - Key: Separate those two
            - USE NON-HEARTBEAT for failure detection

    2.5 Another Probabilistic Approach
        SWIM (Scalable Weekly consistent Infection style Membership protocol)
        - Instead of Heartbeats, use Pings.
        - Send indirect ping

        - Pick one process at Random and send a Ping
        - If ACK received then done.
        - Else Send an indirect ping and wait for an indirect ACK
            - Indirect ping is sent to 'k' other processes
            - Even if One indirect ACK is received then done
        - If no indirect ACK then remove the process

        - THIS WAY, GIVING Pj a TEMPORAL CHANCE AND A SPATIAL CHANCE
            - Temporal Chance: By sending a Second ping
            - Spatial Chance: Indirect path

        Hearbeating vs Swim:

        - Select 'k' different processes by just going over the membership list lineraly
        - Every time after sending 'k' PINGs to identify a faulty process, REORDER the
          membership list. So that next time you will select 'k' different processes

    2.6 Dissemination and Suspension   
        Dissemination:
            - Can be used to suspect if a process has failed
            - Can be used when a PROCESS IS JOINING / LEAVING the protocol

        - Once failure is detected that info should be sent to everyone else

        PROJ
        - Every process MAINTAINS a list of recently joined and recently left processes
        - Piggy back a part of this info along with the ACK, so that others can update it.

        - Piggy back on failure detector messages
            - Useful when SWIM style is used.

        PROJ:
        - Suspension:
            - Insted of directly moving to failed state, move to Suspect state
            - While in suspect state if there is a message from Pj then move back to active state

            PROJ
            - Problem:
                Piggyback between suspected and active state.
                - So have Incarnation numbers.
                - Only Pj can Increment the Incarnation number
                - If someone is suspecting Pj, then Pj will increment its Incarnation number and
                  start sending it out along with ALIVE.
                - IF SUSPICION: HIGHER incarnation numbers override the LOW incarnation numbers
                - IF FAILED, that overrides everything else

Week 3:
P2P Systems:
    - Unstructured P2P systems.
        - Napster, Gnutella

    - Structured P2P systems
        - Chord, Pastry

    NAPSTER:
        - Server stores where the files are located

    Gnutella
        - Five types of message:
            Query, Queryhit, Push, Ping, and Pong

        - No server. All Peers
        - TTL-Restricted

        - Uses Descriptor ID and previous history of messages that are forwarded to avoid
          re-forwarding the same message.

        QUERY HIT:
        - Every PEER keeps tracks of recent query messages and which Peer it got the query from
          This is used for REVERSING the query search i.e. for Query Hit

        TTL:
        - Whenever a message is forwarded by a peer to its neighboring peers,
          TTL is decremented by one.
        - When the TTL hits zero, you do not forward the message anymore.
        - The TTL is set to be a finite number, so that essentially a message doesn't keep circulating around the overlay graph forever.

        Transfer Files:
        - HTTP is used.
        - GET message is used to transfer the files
        - If behind FIREWALL HTTP gets blocked.
          So same concept of Push and Reverse Path Lookup is used.

        PROBLEMS:
        - Ping pong constituted 50% of traffic
          - Even though you recieve multiple pongs, you consolidate and send pongs to your neighbors

    FastTrack:
        - Just like Gnutella but few peers are Supernodes
        - So query just by searching the Supernode instead of the entire network

    BitTorrent:
        - Choking: To make sure too much of upload doesn't happen

    Chord:
        - Distributed Hash table (DHT)
        Performance Concerns:
            - Load Balancing
                - Virtual Nodes (Virtual Ring) assumption to ensure load balancing
            - Fault Tolerance

        - Uses CH (Consistent Hashing) on peer's address
            - Get the hash value and truncate to 'm' bits

    Pastry:
        - Each nodes knows few successors and few predecessors
        -  This is sort of like hypercube routing.
        - So routing is based on prefix matching and turns out to be O(log(N)) because a hypercube with N points is O(log(N)) in diameter.
        - In addition Pastry, unlike Chord, also pays attention to the underlying network.
        - This makes the neighbor edges to be short in the underlying neighboring topology as well. 

        - Select the neighbor with shortest-round-trip time
        - So early hops are much shorter and later hops are much longer

    Kelips:
        - Gives Constant Lookup Cost
        - We don't use a VIRTUAL RING instead use an AFFINITY GROUP

        - Unlike in Chord and Pastry, the files do not get stored at the nodes in the affinity groups.
        - Instead, they get stored at whichever peer uploaded them just like Napster or Gnutella.
        - Instead, what gets stored in an affinity group is meta-information, so you decouple the file replication location from the file querying.

        - You are replicating the info of a file SO WELL that at any points from any of your
          neighbors you can easily get to the file

    Pastry vs Chord vs Kelips
        - So Kelips uses slightly more memory than Chord or Pastry and slightly more background bandwidth, to keep neighbors a fresh, but it has a much shorter lookup cost, which is O(1).
        So while Chord or Pastry have O(log(N)) for all for both memory and lookup, Kelips has O(square root(N)) for memory and O(1) for lookup cost.

Week 4:
    4.1.1 Why Key/Value Store:
    - Data is Large and Unstructured
    - Don't need Joins.
    - Foreign key is rarely used.
    - Write HEAVY
    - RDBMS is useful for READ HEAVY

    NoSQL: Not Only SQL
        - Colum

    4.1.2 Cassandra
        - Developed by FB
        COORDINATOR:
            - The client sends its queries to one of the servers in the system.
            - It need not be one of the replicas. This server is called the coordinator.
            - The coordinator need not be on a per data center basis, it could be on a per client basis or on a per query basis, it doesn't really matter.
            - When Client sends a query, coordinator just forwards it to all or some of the replicas.
            - IMP:  The coordinator again may be per-key, per-client, or per-query.

        PARTITIONER:
            - Mapping from KEY to a SERVER
            - It is used by the Coordinator to find the REPLICA servers to which a query should be forwarded.
            - Different Types:
              Simple Strategy and Network Topology Strategy
              Network Topology Strategy:
                - Go over the ring clockwise until you hit the first SERVER on a different rack

        SNITCHES:
            - To map IP addresses to RACKS and Data Centers

        Writes:
            - Hinted Handoff Mechanism
              - Happens when Client sends a write to a coordinator and the replicas are down
            - The coordinator receives his write, uses the PARTITIONER to find out which of the replicas that map to that particular key
            - Coordinator keeps a copy of the WRITE till the replica that is down comes up
            - This is done for a small duration.

            - Replica logs the write to a diskfile to protect from failures

        MEMTABLE:
            - It is an IN-MEMORY REPRESENTATION of multiple key-value pairs
            - It is the one that holds all key-value pairs
            - it is maintained as WRITE BACK CACHE instead of WRITE THROUGH CACHE

            WriteBack Cache - Storing temporarily in memory
            WriteThrough Cache - Writing directly to disk. Slower

        Access Table OR SSTable:
            - a ROW could be split across multiple access tables.
            - Once Memtable becomes full or reached a timeout, you flush its contents to disk
            - CREATE an ASSIST table or a SORTED String Table. This is stored on the disk.
            - Searching for a KEY in this data file could take long as it has KEY and VALUES

            - So an INDEX file is mainted to Store Key and position of data 
            - Use BLOOM FILTER to store efficiently
            - This helps in telling if the key is present or not

        Membership:
            - Every server maintains a LIST OF ALL OTHER SERVERS in the cluster
            - Uses Gossip style memebership
            - Suspicion mechanism is used to ensure ACCURACY

    4.1.3 CAP Theorem
        Cassandra favors Availability than Consistency (can tolerate a hit)
        
        Quorum:
            - More than 50% of replica should response
            - ANY
            - ALL
            - One : At least one replica. Can't be the coordinator
            - Quorum
            - Local_Quorum
            - Each_Quorum

    4.1.4 Consistency Spectrum
        - Per Key Sequential

        - CRDT - Commutative Replicated Data Type
            - Writes need not be ordered.
            - Writes in any order will give the same result

        - Red-Blue Consistency
            - Blue ops are COMMUTATIVE
            - Red ops should be executed in the same order

        - Causal Consistency:

    4.1.5 HBase
        - API functions:
            - Get/Put
            - Scan (row range, filter)
            - MultiPut

        - Prefers Consistency over Availability

        - HBase Table
            - Split into REGIONS
              Regions are a collection of ROWS
            - You don't want to store the entire ROW. So you split it into REGIONS
            - Column Family:
                - Subset of Columns with similar query pattern

            - One STORE per combination of ColumnFamily + Region

    4.2 Time and Ordering
    4.2.1 Why Synchronization:
        - Violation of Correctness:
            - When you miss the bus
        - Violation of Fairness:
            - When you wait too long for the bus

        - Each system has its own Clock
        - It is easy to ORDER events within a PROCESS
        - But it is difficult to ORDER events ACROSS processes

        Clock Skew vs Clock Drift
        - Clock Skew
            - Difference in Values of two clock
        - Clock Drift:
            - Difference in speed

    4.2.2 Cristian's Algorithm
        - External Synchronization
        - By the time we get the response from External Server, time would have moved on

        IMP:
        - NEVER change the CLOCK to old value
        - Can change the Speed of clock (Both Increase and Decrease)
    
    4.2.3 NTP
        - One of the standard

    4.2.4 Lamport Timestamp
        - Alternative to Synchronizing clocks
        - Order the events

    4.2.5 Vector Clocks

5. Classical Distributed Algorithms
    5.1 Global Snapshots
        Approach 1:
        - Make sure all processes have CLOCKs synced.
        - Every one hour take a snapshot
        Problems:
        - What if a clock skew
        - Cannot capture channel states
        Channel:
            - Communication happening between processees

    5.2 Global Snapshot Algorithm
        - Distributed System moves from One Global State to another ensuring Causal Property

    5.3 Liveness:
            - Something good will eventually happen
            - Distributed computation will terminate eventually
            - TIME_BOUND liveness is difficult to achieve

        Safety:
            - Something BAD will never happen
            - Eg: There is no deadlock in the system
            - No objects orphaned

        - Stable Liveness:
            - Once terminated process should remain that way

        - Stable Non-Safety:
            - Once deadlock remain that way
